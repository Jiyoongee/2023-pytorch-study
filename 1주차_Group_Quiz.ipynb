{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1."
      ],
      "metadata": {
        "id": "2C05h7pOVxzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn9AurwTViMs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "true_b = 1\n",
        "true_w = 2\n",
        "\n",
        "N = 100\n",
        "\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(N,1)  # Uniform\n",
        "epsilon = (0.1 * np.random.randn(N,1))  # standard normal dist  # noise\n",
        "y = true_b + true_w * x + epsilon  # data genneration\n",
        "\n",
        "# Shuffles the indices\n",
        "idx = np.arange(N)\n",
        "split_index = int(N * 0.8)\n",
        "\n",
        "train_idx = idx[:split_index]\n",
        "val_idx = idx[split_index:]\n",
        "\n",
        "# Generates train and validation sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]\n",
        "\n",
        "# create tensor ar GPU:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU가 현재 연결이 되어있다면 cuda를 콜하겠다. GPU에 내 데이터를 올릴거예요.\n",
        "x_train_tensor = torch.as_tensor(x_train).to(device)\n",
        "y_train_tensor = torch.as_tensor(y_train).to(device)\n",
        "\n",
        "x_val_tensor = torch.as_tensor(x_val).to(device)\n",
        "y_val_tensor = torch.as_tensor(y_val).to(device)\n",
        "\n",
        "# train data 로 구한 parameter\n",
        "@timer\n",
        "def train_model_torch(lr = 0.1, epochs=1000):\n",
        "  # Initialize parameters\n",
        "  b_hat = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
        "  w_hat = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # Loss computation\n",
        "    y_hat = b_hat + w_hat * x_train_tensor\n",
        "    error = (y_hat - y_train_tensor)\n",
        "    mse_loss = torch.mean(error ** 2)\n",
        "    # Gradient computation and descent\n",
        "    mse_loss.backward()\n",
        "    with torch.no_grad(): # 경사 하강법 할 땐 auto grad 끄고 하는 게 좋음. 아니면 뻑남\n",
        "      b_hat -= lr * b_hat.grad  # in-place operation 써줘야함.\n",
        "      w_hat -= lr * w_hat.grad\n",
        "    b_hat.grad.zero_() # gradient 초기화 시켜줘야함. 안 하면 누적합으로 계산하기 때문에\n",
        "    w_hat.grad.zero_()\n",
        "\n",
        "  mse_loss = nn.MSELoss()\n",
        "  y_hat = b_hat + w_hat * x_val_tensor\n",
        "  loss = mse_loss(y_hat, y_val_tensor)\n",
        "  return b_hat, w_hat, loss\n",
        "\n",
        "b_hat, w_hat, loss = train_model_torch()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2."
      ],
      "metadata": {
        "id": "oG6vSEJLV8Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# 데이터 로드\n",
        "with open('quiz_data.pkl', 'rb') as f:\n",
        "\tdata = pickle.load(f)\n",
        "\n",
        "x = data[\"x\"]\n",
        "y = data[\"y\"]\n",
        "\n",
        "# Shuffles the indices\n",
        "idx = np.arange(100)\n",
        "split_index = int(100 * 0.8)\n",
        "\n",
        "train_idx = idx[:split_index]\n",
        "val_idx = idx[split_index:]\n",
        "\n",
        "# Generates train and validation sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]\n",
        "\n",
        "# create tensor ar GPU:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU가 현재 연결이 되어있다면 cuda를 콜하겠다. GPU에 내 데이터를 올릴거예요.\n",
        "x_train_tensor = torch.as_tensor(x_train).to(device)\n",
        "y_train_tensor = torch.as_tensor(y_train).to(device)\n",
        "\n",
        "x_val_tensor = torch.as_tensor(x_val).to(device)\n",
        "y_val_tensor = torch.as_tensor(y_val).to(device)\n",
        "\n",
        "sum = 0\n",
        "for i in range(10):\n",
        "\n",
        "  def train_model_torch(lr = 0.1, epochs=1000):\n",
        "    # Initialize parameters\n",
        "    b_hat = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
        "    w_hat = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Loss computation\n",
        "      y_hat = b_hat + w_hat * x_train_tensor\n",
        "      error = (y_hat - y_train_tensor)\n",
        "      mse_loss = torch.mean(error ** 2)\n",
        "      # Gradient computation and descent\n",
        "      mse_loss.backward()\n",
        "      with torch.no_grad(): # 경사 하강법 할 땐 auto grad 끄고 하는 게 좋음. 아니면 뻑남\n",
        "        b_hat -= lr * b_hat.grad  # in-place operation 써줘야함.\n",
        "        w_hat -= lr * w_hat.grad\n",
        "      b_hat.grad.zero_() # gradient 초기화 시켜줘야함. 안 하면 누적합으로 계산하기 때문에\n",
        "      w_hat.grad.zero_()\n",
        "\n",
        "    mse_loss = nn.MSELoss()\n",
        "    y_hat = b_hat + w_hat * x_val_tensor\n",
        "    loss = mse_loss(y_hat, y_val_tensor)\n",
        "    return b_hat, w_hat, loss\n",
        "\n",
        "  b_hat, w_hat, loss = train_model_torch()\n",
        "  sum += loss\n",
        "\n",
        "print(sum / 10)"
      ],
      "metadata": {
        "id": "FPUnV-1QV-fZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}